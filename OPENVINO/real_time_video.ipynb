{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time \n",
    "import cv2\n",
    "# for openvino\n",
    "## before this should run setupvar.bat for openvino\n",
    "from PIL import Image\n",
    "from openvino import inference_engine as ie\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "import scipy.io as sio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_3': <openvino.inference_engine.ie_api.DataPtr object at 0x00000240D10B4390>}\n",
      "{'strided_slice/Squeeze_shrink': <openvino.inference_engine.ie_api.DataPtr object at 0x00000240D10B4390>}\n",
      "{'zero_padding2d_1_input': <openvino.inference_engine.ie_api.DataPtr object at 0x00000240D10B4390>}\n",
      "{'strided_slice/Squeeze_shrink': <openvino.inference_engine.ie_api.DataPtr object at 0x00000240D10B4390>}\n"
     ]
    }
   ],
   "source": [
    "# Plugin initialization for specified device and load extensions library if specified.\n",
    "plugin_dir = None\n",
    "model_xml = './vk/fall_detection_shirui-master/ir_model/shirui_saved.xml'\n",
    "model_bin = './vk/fall_detection_shirui-master/ir_model/shirui_saved.bin'\n",
    "# Devices: GPU (intel), CPU, MYRIAD\n",
    "plugin = IEPlugin(\"CPU\", plugin_dirs=plugin_dir)\n",
    "# Read IR\n",
    "net = IENetwork(model=model_xml, weights=model_bin)\n",
    "print(net.inputs)\n",
    "print(net.outputs)\n",
    "input_blob = next(iter(net.inputs))\n",
    "out_blob = next(iter(net.outputs))\n",
    "# Load network to the plugin\n",
    "exec_net = plugin.load(network=net)\n",
    "del net\n",
    "\n",
    "# Plugin initialization for specified device and load extensions library if specified.\n",
    "plugin_dir_f = None\n",
    "model_xml_f = './vk/fall_detection_shirui-master/ir_model/shirui_featrue_extractor_saved.xml'\n",
    "model_bin_f = './vk/fall_detection_shirui-master/ir_model/shirui_featrue_extractor_saved.bin'\n",
    "# Devices: GPU (intel), CPU, MYRIAD\n",
    "plugin_f = IEPlugin(\"CPU\", plugin_dirs=plugin_dir_f)\n",
    "# Read IR\n",
    "net_f = IENetwork(model=model_xml_f, weights=model_bin_f)\n",
    "print(net_f.inputs)\n",
    "print(net_f.outputs)\n",
    "input_blob_f = next(iter(net_f.inputs))\n",
    "out_blob_f = next(iter(net_f.outputs))\n",
    "# Load network to the plugin\n",
    "exec_featrue_extrctor = plugin_f.load(network=net_f)\n",
    "del net_f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define functions to predict\n",
    "\n",
    "def test_video_realtime(exec_featrue_extrctor,flow, ground_truth,mean_file):\n",
    "    # Load the mean file to subtract to the images\n",
    "    # flow is of shape(224,224,20,n)\n",
    "    num_features = 4096\n",
    "    d = sio.loadmat(mean_file)\n",
    "    flow_mean = d['image_mean']\n",
    "    \n",
    "    flow = flow - np.tile(flow_mean[...,np.newaxis], (1, 1, 1, flow.shape[3]))\n",
    "    flow = np.transpose(flow, (3, 0, 1, 2)) \n",
    "    predictions = np.zeros((flow.shape[0], num_features), dtype=np.float64)\n",
    "    truth = np.zeros((flow.shape[0], 1), dtype=np.float64)\n",
    "      \n",
    "        # Process each stack: do the feed-forward pass\n",
    "    for i in range(flow.shape[0]):\n",
    "        pro_Img = (flow[i]).transpose((2, 0, 1))\n",
    "        prediction = exec_featrue_extrctor.infer(inputs={input_blob_f: np.expand_dims(pro_Img,0)})\n",
    "        # Access the results and get the index of the highest confidence score\n",
    "        output_node_name = list(prediction.keys())[-1]\n",
    "        prediction = prediction[output_node_name]\n",
    "        predictions[i, ...] = prediction\n",
    "        truth[i] = ground_truth\n",
    "        \n",
    "    return predictions, truth\n",
    "\n",
    "\n",
    "def real_time_predict(exec_featrue_extrctor,exec_net,flow_data):\n",
    "    # flow data is of shape (224x,224y,20,n)\n",
    "    num_features = 4096\n",
    "    mean_file = './vk/fall_detection_shirui-master/flow_mean.mat'\n",
    "    save_features = False\n",
    "    save_plots = True\n",
    "\n",
    "    do_training = False\n",
    "    do_testing = True\n",
    "    compute_metrics = True\n",
    "    threshold = 0.5\n",
    "    ground_truth = 0\n",
    "    num_features = 4096\n",
    "\n",
    "    # fold_best_model_path = './urfd_fold_5.h5'\n",
    "    L = 10\n",
    "\n",
    "    X2,truth = test_video_realtime(exec_featrue_extrctor,flow_data, ground_truth,mean_file)\n",
    "\n",
    "    predicted = []\n",
    "    for i in range(len(X2)):\n",
    "        predicted.append(  exec_net.infer(inputs={input_blob:np.asarray(X2[i])}) )\n",
    "    return predicted\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.8910168], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n",
      "[{'strided_slice/Squeeze_shrink': array([0.9999998], dtype=float32)}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## set up video stream\n",
    "vs = VideoStream(src = 1).start()\n",
    "time.sleep(2.0) # wait for video to start\n",
    "fps = FPS().start()\n",
    "\n",
    "cur_images = []\n",
    "stacked_flow = []\n",
    "while 1:\n",
    "    # capture and show the frame\n",
    "    frame = vs.read()\n",
    "    # print(frame)\n",
    "    cv2.imshow(\"frame:\",frame)\n",
    "    \n",
    "    \n",
    "    ###### get the img and the flow ############\n",
    "    # here name frame needs to contain the current frame in the video\n",
    "    \n",
    "    # cur_image stores the recent 2 images (in grey reseized format)\n",
    "    image = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_AREA)  \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if len(cur_images) < 2:\n",
    "        cur_images.append(image)\n",
    "    else:\n",
    "        cur_images.append(image)\n",
    "        cur_images.pop(0)\n",
    "     \n",
    "    if len(cur_images) == 2:\n",
    "        optical_flow = cv2.DualTVL1OpticalFlow_create()\n",
    "        flow = optical_flow.calc(cur_images[0], cur_images[1], None)\n",
    "\n",
    "        flow[..., 0] = cv2.normalize(flow[..., 0], None, 0, 255, cv2.NORM_MINMAX)\n",
    "        flow[..., 1] = cv2.normalize(flow[..., 1], None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        # flow contains the current optival flow\n",
    "        # while stacked flow contains the flow of recent 10 frame\n",
    "        stacked_flow.append(flow[..., 0])\n",
    "        stacked_flow.append(flow[..., 1])\n",
    "        \n",
    "        if len(stacked_flow) > 20:\n",
    "            del stacked_flow[0:2]\n",
    "\n",
    "    ##### here is the openvino part ###\n",
    "        if len(stacked_flow) == 20:\n",
    "            flow_data = np.array(stacked_flow)\n",
    "            flow_data = flow_data.transpose(1,2,0)\n",
    "            flow_data = (np.array(stacked_flow)).reshape(224,224,20,1)\n",
    "\n",
    "            cur_predict = real_time_predict(exec_featrue_extrctor,exec_net,flow_data)\n",
    "\n",
    "            print(cur_predict)\n",
    "\n",
    "    #### openvino end\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    fps.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
